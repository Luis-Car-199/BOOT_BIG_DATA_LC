Ejercicios comentados:

Entrega 1:

Creamos un cluster hadoop desde google cloud data proc. Esto lo hago para tener un entorno distribuido y poder ejecutar trabajos hadoop como el procesamiento de grandes volumenes datos.
Por otro lado creo un bucket para hacer una prueba de descarga desde google cloud al sistema de archivos del cluster hadoop. Primero subo los jar de configuracion al bucket y despues desde
la consola de del nodo maestro del cluster descargo los archivos que hay en el bucket.

Entrega 2:

Configuro un server elastic. Creo una maquina virtual independiente para conectarlo depsues al cluster y  descargo, instalo y configuro  elastic search y kivana y los reiniciamos para
que queden listo para trabajar con ellos. Dejo creados tambien los firewall para poder acceder sin restricciones desde el cluster y desde mi pc.

Entrega  3:

Configuro el cluster hadoop Hive para poder hacer consultas y analisis de los datos almacenados en HDFS en SQL

Entrega 4:

Pruebo primero creando un indice en elastic y a√±ado los datos a traves de nuestro cluster al indice de elastic. finalemente compruebo la inserccion de datos.


